{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_md"
   },
   "source": [
    "# üß†üéôÔ∏è Psychology Voice Assistant\n",
    "\n",
    "Welcome to the **Psychology Voice Assistant** notebook. This tool simulates a session with **Dr. Julian**, a positive psychologist specializing in Transition Resilience.\n",
    "\n",
    "### Features:\n",
    "- üó£Ô∏è **Voice Input**: Talk to the AI using your browser microphone.\n",
    "- ü§ñ **Local LLM**: Uses Ollama (running locally/on Colab) for privacy and speed.\n",
    "- üîä **Voice Response**: The AI replies with audio.\n",
    "- üíæ **Chat History**: Remembers your conversation context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 1. üõ†Ô∏è System Setup & Dependencies\n",
    "Installing necessary system libraries for audio processing and Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sys_install"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies for audio (PortAudio, eSpeak, FFmpeg)\n",
    "!sudo apt-get install -y portaudio19-dev espeak ffmpeg\n",
    "\n",
    "# Install Ollama script\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pip_install"
   },
   "outputs": [],
   "source": [
    "# Install Python libraries\n",
    "!pip install streamlit speechrecognition pyttsx3 langchain langchain-community langchain-core langchain-ollama pyaudio streamlit_jupyter gTTS pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_header"
   },
   "source": [
    "## 2. üìö Imports & Configuration\n",
    "Importing libraries and setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import io\n",
    "import tempfile\n",
    "from base64 import b64decode\n",
    "\n",
    "# Web & UI\n",
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher\n",
    "\n",
    "# Audio Processing\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "from IPython.display import Audio, display, Javascript\n",
    "from google.colab import output\n",
    "\n",
    "# AI & LangChain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Patch Streamlit to work in Jupyter/Colab\n",
    "StreamlitPatcher().jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_vars"
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"gpt-oss:20b\"  # Options: 'llama3', 'phi', 'gpt-oss:20b'\n",
    "print(f\"Selected Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ollama_header"
   },
   "source": [
    "## 3. üöÄ Ollama Server Initialization\n",
    "We need to start the Ollama server in the background and pull the requested model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ollama_start"
   },
   "outputs": [],
   "source": [
    "print(\"Starting Ollama server...\")\n",
    "\n",
    "# Start Ollama serve as a background process\n",
    "process = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "time.sleep(5)  # Give it a moment to initialize\n",
    "\n",
    "print(f\"Pulling model '{MODEL_NAME}' (this may take a few minutes)...\")\n",
    "!ollama pull {MODEL_NAME}\n",
    "\n",
    "# specific env var for CUDA if needed\n",
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ollama_list"
   },
   "outputs": [],
   "source": [
    "# Verify installed models\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "components_header"
   },
   "source": [
    "## 4. üß© Core Components: Audio & TTS\n",
    "Functions to handle browser-based audio recording and Text-to-Speech generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tts_engine"
   },
   "outputs": [],
   "source": [
    "class ColabEngine:\n",
    "    \"\"\"Custom TTS engine wrapper using gTTS for Colab environment.\"\"\"\n",
    "    def setProperty(self, name, value):\n",
    "        pass \n",
    "\n",
    "    def say(self, text):\n",
    "        try:\n",
    "            tts = gTTS(text=text, lang='en')\n",
    "            tts.save(\"response.mp3\")\n",
    "            # IPyWidgets Audio player\n",
    "            display(Audio(\"response.mp3\", autoplay=True))\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating audio: {e}\")\n",
    "\n",
    "    def runAndWait(self):\n",
    "        pass\n",
    "\n",
    "engine = ColabEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "js_recorder"
   },
   "outputs": [],
   "source": [
    "# Javascript code to capture microphone audio in the browser\n",
    "RECORD_JS = \"\"\"\n",
    "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "def record_audio(sec=5):\n",
    "    \"\"\"\n",
    "    Injects JS to record audio for `sec` seconds.\n",
    "    Returns: Path to the temporary WAV file.\n",
    "    \"\"\"\n",
    "    display(Javascript(RECORD_JS))\n",
    "    print(f\"üéôÔ∏è Recording for {sec} seconds...\")\n",
    "    s = output.eval_js('record(%d)' % (sec*1000))\n",
    "    print(\"‚úÖ Recording finished.\")\n",
    "    b = b64decode(s.split(',')[1])\n",
    "\n",
    "    # Convert webm to wav\n",
    "    audio = AudioSegment.from_file(io.BytesIO(b))\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
    "        audio.export(f.name, format=\"wav\")\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "listen_wrapper"
   },
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def listen_browser():\n",
    "    \"\"\"Capture audio and transcribe using Google Speech Recognition.\"\"\"\n",
    "    try:\n",
    "        audio_file = record_audio(sec=5)\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            query = recognizer.recognize_google(audio_data)\n",
    "            st.write(f\"**You:** {query}\")\n",
    "            return query.lower()\n",
    "    except sr.UnknownValueError:\n",
    "        st.warning(\"üòï Sorry, I couldn't understand. Please speak clearly.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ö†Ô∏è Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai_setup_header"
   },
   "source": [
    "## 5. üß† AI Agent Setup (LangChain + Ollama)\n",
    "Defining the persona \"Dr. Julian\" and initializing the conversation chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "persona_def"
   },
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = OllamaLLM(model=MODEL_NAME)\n",
    "\n",
    "# Initialize Chat History in Session State\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = ChatMessageHistory()\n",
    "\n",
    "# Persona Template\n",
    "template = \"\"\"\n",
    "### ROLE\n",
    "You are Dr. Julian, a warm, empathetic, and unwavering Positive Psychologist specializing in \"Transition Resilience.\" \n",
    "Your goal is to help people navigating the stress of relocation to see the experience as a profound opportunity for personal growth and adventure.\n",
    "\n",
    "### GUIDELINES\n",
    "1. VALIDATE: Acknowledge the difficulty of the move (homesickness, fatigue, confusion).\n",
    "2. REFRAME: Always shift the narrative toward \"The New Chapter\" and \"Discovery.\"\n",
    "3. TONE: Use words like \"Courageous,\" \"Growth,\" \"Potential,\" and \"Roots.\"\n",
    "4. STYLE: Keep responses conversational, narrative, and grounded in psychological strength.\n",
    "5. FORMATTING: Use Markdown. Do NOT use tables.\n",
    "\n",
    "### CONTEXT\n",
    "History:\n",
    "{chat_history}\n",
    "\n",
    "User Input:\n",
    "{question}\n",
    "\n",
    "### DR. JULIAN'S RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chain_runner"
   },
   "outputs": [],
   "source": [
    "def run_chain(question):\n",
    "    \"\"\"Execute the LLM chain and update history.\"\"\"\n",
    "    # Format history as text\n",
    "    chat_history_text = \"\\n\".join([\n",
    "        f\"{msg.type.capitalize()}: {msg.content}\" for msg in st.session_state.chat_history.messages\n",
    "    ])\n",
    "    \n",
    "    # Invoke LLM\n",
    "    response = llm.invoke(prompt.format(chat_history=chat_history_text, question=question))\n",
    "    \n",
    "    # Save messages\n",
    "    st.session_state.chat_history.add_user_message(question)\n",
    "    st.session_state.chat_history.add_ai_message(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "app_header"
   },
   "source": [
    "## 6. ‚ñ∂Ô∏è Main Application\n",
    "Run the cell below to start the interaction loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_loop"
   },
   "outputs": [],
   "source": [
    "st.title(\"üß†üéôÔ∏è Dr. Julian: AI Voice Assistant\")\n",
    "st.markdown(\"Tap the play arrow on the cell to run. Then wait for the prompt to speak.\")\n",
    "\n",
    "# Start Interaction\n",
    "user_input = listen_browser()\n",
    "\n",
    "if user_input:\n",
    "    # Generate Response\n",
    "    with st.spinner(\"Dr. Julian is thinking...\"):\n",
    "        ai_response = run_chain(user_input)\n",
    "    \n",
    "    # Display Response\n",
    "    st.markdown(f\"**Dr. Julian:** {ai_response}\")\n",
    "    \n",
    "    # Speak Response\n",
    "    engine.say(ai_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
